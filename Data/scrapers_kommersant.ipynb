{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d7530b",
   "metadata": {},
   "source": [
    "# [Kommersant](https://www.kommersant.ru/finance?from=burger) parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcaaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import pickle\n",
    "import time\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b47c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraper's options\n",
    "options = Options()\n",
    "\n",
    "# Fully load the page to avoid some problems\n",
    "options.page_load_strategy = 'normal'\n",
    "\n",
    "# To avoid scraper detection and other problems\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Some other features\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_argument(\"--mute-audio\")\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--headless=new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00863509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly we want to get all news' links\n",
    "def parse_links_kommersant(webdriver_options, dates, lst_links):\n",
    "    driver = webdriver.Chrome(options=webdriver_options)\n",
    "\n",
    "    for i, date in tqdm(enumerate(dates)):\n",
    "        driver.get(f'https://www.kommersant.ru/archive/rubric/40/day/{date}')\n",
    "        elements = driver.find_elements(By.XPATH, '//div[@class=\"rubric_lenta\"]//div//h2//a')\n",
    "        for element in elements:\n",
    "            lst_links.append(element.get_attribute('href'))\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'Numbers of links on {i} iteration: {len(lst_links)}')\n",
    "        \n",
    "    time.sleep(1)\n",
    "    driver.close()\n",
    "    time.sleep(1)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf26d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_span = abs((date(2019, 1, 1) - date(2023, 12, 31)).days)\n",
    "dates_lst = [(date(2019, 1, 1) + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(dates_span)]\n",
    "lst_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd5699b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parse_links_kommersant(webdriver_options=options,\n",
    "                       dates=dates_lst,\n",
    "                       lst_links=lst_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30c228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that we do not have duplicaet links\n",
    "assert len(lst_links) == len(set(lst_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee239943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are duplicates that is why we take a set of lst_links\n",
    "\n",
    "# Writing links to file to avoid parcing each time\n",
    "with open(r'kommersant_links.txt', 'w') as file:\n",
    "    for el in set(lst_links):\n",
    "        file.write(\"%s\\n\" % el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e043e6b",
   "metadata": {},
   "source": [
    "### Output format (columns)\n",
    "1. id (pandas.DF) - default column\n",
    "\n",
    "2. website (where the news were retreived)\n",
    "\n",
    "3. section of the website (where the news were retreived)\n",
    "\n",
    "4. url (of the news)\n",
    "\n",
    "5. header (of the news)\n",
    "\n",
    "6. body (of the news)\n",
    "\n",
    "7. date (of the news)\n",
    "\n",
    "8. tags/key_words (of the news, if there are any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6f073",
   "metadata": {},
   "source": [
    "### This dataset\n",
    "1. default\n",
    "\n",
    "2. Kommersant (same for each url)\n",
    "\n",
    "3. section (vary for each url)\n",
    "\n",
    "4. url (unique for each url)\n",
    "\n",
    "5. header (unique for each url)\n",
    "\n",
    "6. body (unique for each url)\n",
    "\n",
    "7. date (unique for each url)\n",
    "\n",
    "8. tags/key_words (empty for each url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1816eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_news_on_list_of_links(webdriver_options, urls_lst,\n",
    "                                body_lst, header_lst, date_lst,\n",
    "                                section_lst, new_urls_lst):\n",
    "    \n",
    "    regex_for_external_links = r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)'\n",
    "\n",
    "    driver = webdriver.Chrome(options=webdriver_options)\n",
    "\n",
    "    for i in tqdm(range(len(urls_lst))):\n",
    "        try:\n",
    "            # For debugging purposes\n",
    "            if i % 200 == 0:\n",
    "                print(f'length of body_lst: {len(body_lst)}')\n",
    "                print(f'length of header_lst: {len(header_lst)}')\n",
    "                print(f'length of date_lst: {len(date_lst)}')\n",
    "                print(f'length of section_lst: {len(section_lst)}')\n",
    "                print(f'length of new_urls_lst: {len(new_urls_lst)}')\n",
    "\n",
    "            driver.get(urls_lst[i])\n",
    "\n",
    "            body = driver.find_element(By.XPATH, '//div[@class=\"doc__body\"]/div[2]').text\n",
    "            # Remove external links as we will not use them\n",
    "            body_without_external_links = re.sub(regex_for_external_links, '', body)\n",
    "#             print(f'BODY: {body_without_external_links} \\n')\n",
    "            if body_without_external_links.strip() == '':\n",
    "                continue\n",
    "            \n",
    "            header = driver.find_element(By.XPATH, '//header/h1').text\n",
    "#             print(f'HEADER: {header} \\n')\n",
    "\n",
    "            date = driver.find_element(By.XPATH, '//div[@class=\"doc_header__time\"]//time').text\n",
    "#             print(f'DATE: {date} \\n')\n",
    "\n",
    "            section = driver.find_element(By.XPATH, '//ul[@class=\"crumbs\"]//li//a').text\n",
    "#             print(f'SECTION: {section} \\n')\n",
    "\n",
    "            body_lst.append(body)\n",
    "            header_lst.append(header)\n",
    "            date_lst.append(date)\n",
    "            section_lst.append(section)\n",
    "            new_urls_lst.append(urls_lst[i])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    time.sleep(1)\n",
    "    driver.close()\n",
    "    time.sleep(1)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d19b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_lst = []\n",
    "body_lst = []\n",
    "header_lst = []\n",
    "date_lst = []\n",
    "section_lst = []\n",
    "new_urls_lst = []\n",
    "\n",
    "# Reading links from the file\n",
    "with open(r'kommersant_links.txt', 'r') as file:\n",
    "    urls_lst = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3625d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse all links collected\n",
    "parse_news_on_list_of_links(webdriver_options=options,\n",
    "                            urls_lst=urls_lst,\n",
    "                            body_lst=body_lst,\n",
    "                            header_lst=header_lst,\n",
    "                            date_lst=date_lst,\n",
    "                            section_lst=section_lst,\n",
    "                            new_urls_lst=new_urls_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a98a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some additional check\n",
    "print(len(urls_lst), '\\n')\n",
    "\n",
    "print(len(body_lst))\n",
    "print(len(header_lst))\n",
    "print(len(date_lst))\n",
    "print(len(section_lst))\n",
    "print(len(new_urls_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f758b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns with the same values for each url\n",
    "website_lst = ['Kommersant' for _ in range(len(body_lst))]\n",
    "key_words_lst = [[] for _ in range(len(body_lst))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f586b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crate pandas DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'website': website_lst,\n",
    "        'section': section_lst, \n",
    "        'url': new_urls_lst,\n",
    "        'header': header_lst,\n",
    "        'body': body_lst,\n",
    "        'date': date_lst,\n",
    "        'key_words': key_words_lst\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1735459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parced data\n",
    "df.to_parquet('kommersant_parced_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f49bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05588246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "df = pd.read_parquet('kommersant_parced_data.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4830392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
